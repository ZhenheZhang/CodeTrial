{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def getPredict(self, x, w, b):\n",
    "        y_head = np.matmul(w, x.transpose()) + b\n",
    "        act = Activation()\n",
    "        y_head = act.sigmoid(y_head)\n",
    "        return np.where(y_head > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self) -> None:\n",
    "        self.value = 0.0\n",
    "\n",
    "    def L1(self, y, y_pred, N): #MAE\n",
    "        self.value += np.abs(y - y_pred) * (1 / N)\n",
    "\n",
    "    def BinaryCrossEntropy(self, y, y_pred, N):\n",
    "        epsilon = 1e-5\n",
    "        self.value += -(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon)) * (1 / N)\n",
    "\n",
    "\n",
    "class Activation():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "            \n",
    "class Optimizer():\n",
    "    def __init__(self) -> None:\n",
    "        self.w_gradient = 0.0\n",
    "        self.b_gradient = 0.0\n",
    "\n",
    "    def SGD(self, x, y, y_pred, N):\n",
    "        if y - y_pred >= 0:\n",
    "            self.w_gradient += -x * (1 / N)\n",
    "            self.b_gradient += -1 * (1 / N)\n",
    "        elif y - y_pred < 0:\n",
    "            self.w_gradient += x * (1 / N)\n",
    "            self.b_gradient += 1 * (1 / N)\n",
    "        return [self.w_gradient, self.b_gradient]\n",
    "\n",
    "    def Update(self, w_in, b_in, learning_rate):\n",
    "        w_update = w_in - (learning_rate * self.w_gradient)\n",
    "        b_update = b_in - (learning_rate * self.b_gradient)\n",
    "        return[w_update, b_update]\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, data, lr, iter_num, w_init, b_init):\n",
    "        self.train_data = data\n",
    "        self.learning_rate = lr\n",
    "        self.max_iter_num = iter_num\n",
    "        self.init_weight = w_init\n",
    "        self.init_bias = b_init\n",
    "        self.weight_dim = w_init.shape[0]\n",
    "\n",
    "    def train(self):\n",
    "        w_upt = self.init_weight\n",
    "        b_upt = self.init_bias\n",
    "        for i in range(self.max_iter_num):\n",
    "            [w_upt, b_upt], epoch_loss = self.epoch_train(w_iter=w_upt, b_iter=b_upt)\n",
    "            if i % 500 == 0:\n",
    "                print(\"iteration[{}], loss={} val_acc is {:.4f}\".format(i, epoch_loss, self.calcValAcc(w=w_upt, b=b_upt)))\n",
    "        return [w_upt, b_upt]\n",
    "\n",
    "\n",
    "    def epoch_train(self, w_iter, b_iter):\n",
    "        N = float(len(self.train_data))\n",
    "        model = Model()\n",
    "        loss = Loss()\n",
    "        optim = Optimizer()\n",
    "\n",
    "        for index in range(0, len(self.train_data)):\n",
    "            x = self.train_data[index, 0 : self.weight_dim]\n",
    "            y = self.train_data[index, -1]\n",
    "\n",
    "            # Get prediction\n",
    "            y_pred = model.getPredict(x, w_iter, b_iter)\n",
    "            # Calc gradient\n",
    "            optim.SGD(x, y, y_pred, N)\n",
    "            # Calc BinaryCrossEntropy loss\n",
    "            loss.BinaryCrossEntropy(y, y_pred, N)\n",
    "\n",
    "        # Update weight and bias\n",
    "        update = optim.Update(w_iter, b_iter, self.learning_rate)\n",
    "        return update, loss.value\n",
    "\n",
    "\n",
    "    def calcValAcc(self, w, b):\n",
    "        model = Model()\n",
    "        val_err = 0.0\n",
    "\n",
    "        for index in range(0, len(self.train_data)):\n",
    "            x = self.train_data[index, 0 : self.weight_dim]\n",
    "            y = self.train_data[index, -1]\n",
    "            y_pred = model.getPredict(x, w, b)\n",
    "            val_err += np.abs(y - y_pred)\n",
    "        val_acc = 1 - (val_err / float(len(self.train_data)))\n",
    "        # # Visualization\n",
    "        # plt.cla()\n",
    "        # plt.scatter(self.train_data[:, 0], self.train_data[:, 1])\n",
    "        # plt.plot(self.train_data[:, 0], model.getPredict(self.train_data[:, 0], w, b), 'r-', lw=5)\n",
    "        # plt.show()\n",
    "        return val_acc\n",
    "\n",
    "    def calcTestAcc(self, w, b, test_data):\n",
    "        model = Model()\n",
    "        val_err = 0.0\n",
    "\n",
    "        for index in range(0, len(test_data)):\n",
    "            x = test_data[index, 0 : self.weight_dim]\n",
    "            y = test_data[index, -1]\n",
    "            y_pred = model.getPredict(x, w, b)\n",
    "            val_err += np.abs(y - y_pred)\n",
    "        val_acc = 1 - (val_err / float(len(test_data)))\n",
    "        # # Visualization\n",
    "        # plt.cla()\n",
    "        # plt.scatter(self.train_data[:, 0], self.train_data[:, 1])\n",
    "        # plt.plot(self.train_data[:, 0], model.getPredict(self.train_data[:, 0], w, b), 'r-', lw=5)\n",
    "        # plt.show()\n",
    "        return val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at weight=[0. 0.] bias=0.0 val_acc=0.4125\n",
      "### START TRAINING ###\n",
      "iteration[0], loss=6.763839585690638 val_acc is 0.5875\n",
      "iteration[500], loss=4.029517412772082 val_acc is 0.6500\n",
      "iteration[1000], loss=4.029517412772082 val_acc is 0.6500\n",
      "iteration[1500], loss=4.029517412772082 val_acc is 0.6500\n",
      "iteration[2000], loss=4.029517412772082 val_acc is 0.6500\n",
      "iteration[2500], loss=4.029517412772082 val_acc is 0.6500\n",
      "iteration[3000], loss=4.029517412772082 val_acc is 0.6500\n",
      "iteration[3500], loss=4.029517412772082 val_acc is 0.6500\n",
      "iteration[4000], loss=4.029517412772082 val_acc is 0.6500\n",
      "iteration[4500], loss=4.029517412772082 val_acc is 0.6500\n",
      "iteration[5000], loss=4.029517412772082 val_acc is 0.6500\n",
      "### TRAINING STOPPED ###\n",
      "Trained 5001 iterations: weight=[0.12827876 0.09962278] bias=0.15005750000000834 val_acc=0.65 test_acc=0.65\n"
     ]
    }
   ],
   "source": [
    "def csv_parser(csv_file: str, split_percent=1.0, seed=42) -> Tuple[List[List[float]], List[float], List[List[float]], List[float]]:\n",
    "    rand = random.Random(seed)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    with open(csv_file, newline='') as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            x_value = [float(row[1]), float(row[2])]\n",
    "            y_value = float(row[3])\n",
    "            if rand.random() <= split_percent:\n",
    "                x_train.append(x_value)\n",
    "                y_train.append(y_value)\n",
    "            else:\n",
    "                x_test.append(x_value)\n",
    "                y_test.append(y_value)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def main():\n",
    "    x_train, y_train, x_test, y_test = csv_parser(csv_file='../data/LogisticRegression_data.csv', split_percent=0.8)\n",
    "    x_train = np.array(x_train, dtype=float)\n",
    "    y_train = np.array(y_train, dtype=float)\n",
    "    x_test = np.array(x_test, dtype=float)\n",
    "    y_test = np.array(y_test, dtype=float)\n",
    "\n",
    "    # data simulation\n",
    "    w_tgt = np.array([0.5, 0.3])\n",
    "    b_tgt = 0.1\n",
    "\n",
    "    # prepare train data\n",
    "    points_train = np.zeros((x_train.shape[0], x_train.shape[1] + 1))\n",
    "    points_train[:, 0 : x_train.shape[1]] = x_train\n",
    "    points_train[:, -1] = y_train\n",
    "\n",
    "    # parameters setup\n",
    "    learning_rate = 0.01\n",
    "    w_init = np.zeros_like(w_tgt)\n",
    "    b_init = 0.0\n",
    "    inter_num = 5001\n",
    "    lr_trainer = Trainer(points_train, learning_rate, inter_num, w_init, b_init)\n",
    "\n",
    "    # print\n",
    "    print(\"Starting gradient descent at weight={} bias={} val_acc={}\".format(w_init, b_init, lr_trainer.calcValAcc(w_init, b_init)))\n",
    "\n",
    "    # training loop\n",
    "    print(\"### START TRAINING ###\")\n",
    "    [w, b] = lr_trainer.train()\n",
    "    print(\"### TRAINING STOPPED ###\")\n",
    "    # prepare test data\n",
    "    points_test = np.zeros((x_test.shape[0], x_test.shape[1] + 1))\n",
    "    points_test[:, 0 : x_test.shape[1]] = x_test\n",
    "    points_test[:, -1] = y_test\n",
    "    print(\"Trained {} iterations: weight={} bias={} val_acc={} test_acc={}\".format(inter_num, w, b, lr_trainer.calcValAcc(w, b), lr_trainer.calcTestAcc(w, b, test_data=points_test)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f46f602a9fb80e9e2e49bf94182631528f06d646b9958424248b38c6e6bffff3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
